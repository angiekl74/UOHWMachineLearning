{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This example was focused on the TITANIC dataset.  Therefore all code will be based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('titanic.csv'). \n",
    "# df.shape\n",
    "# df.columns\n",
    "# df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Selecting all rows and a handful of features.  We are trying to predict who SURVIVED by using the following\n",
    "# features: [\"Pclass\", \"Embarked\", \"Sex\"]\n",
    "# Step 2 - getting rid of null rows in \"Embarked\"\n",
    "\n",
    "# df = df.loc[df.Embarked.notna(), [\"Survived\", \"Pclass\", \"Embarked\", \"Sex\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - run df.shape again to double check that filters took and no null values\n",
    "\n",
    "# df.shape\n",
    "# df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Cross validate a model that predicts SURVIVE using only Pclass. Then will use\n",
    "# pipeline, one hot encoder and column transformer for Sex and Embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validate model (that predicts SURVIVE using only Pclass feature) using Log Reg (it is a classification problem). The point of cross validation is to EVALUATE your model so you can decide if you are building a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - define X and y.  Remember, X needs to be 2 dimensional array...y can be 1\n",
    "\n",
    "# X = df.loc[:, [\"Pclass\"]]\n",
    "# y = df.Survived\n",
    "# X.shape\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Logistic Regression\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a - Calculate cross_val_score. (cv is the number of folds.  Mean is the mean of the 5 folds cross validation)\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# cross_val_score(logreg, X, y, cv=5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b - compare this to null accuracy.  It is the accuracy you would get by PREDICTING the MOST FREQUENT class.\n",
    "# Your cross_val_score should generally beat the null accuracy\n",
    "\n",
    "# y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding (also known as dummy encoding) CATEGORICAL features (it needs to be numeric!). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why use OneHotEncoder versus Pandas get_dummy?  \n",
    "1. Because OneHotEncoder DOES NOT effect the width of your original df.\n",
    "2. You don't have to worry about preprocessing NEW data as it comes in. For example: You won't have problems if your NEW data has different categories than your TRAINING data.  What happens if your TRAINING data has (C,Q,V in Embarked) and your NEW data has (only C and Q)?  The shape is wrong!!\n",
    "3. You can do a GRIDSEARCH with both model parameters and preprocessing parameters\n",
    "4. In some cases, preprocessing OUTSIDE sklearn can make cross validation scores LESS REALIABLE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - OneHotEncode sex column\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# ohe = OneHotEncoder()\n",
    "# ohe.fit_transform(df[[\"Sex\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - To see the CATEGORIES that OHE did and make sure its correct\n",
    "\n",
    "# ohe.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy encoding multiple categorical features in one go without changing Pclass feature (using make_column_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Redefine X to have all 3 features\n",
    "\n",
    "# X = df.drop(\"Survivied\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Using make_column_transformer to transform the 2 categorical features and leave Pclass alone. Using\n",
    "# column transformer to do ALL THE PREPROCESSING at the same time.  I think there are other features.\n",
    "\n",
    "# from sklearn.compose import make_column_transformer\n",
    "# column_transform = make_column_transformer(\n",
    "#                                 (OneHotEncoder(), [\"Sex\", \"Embarked\"]),\n",
    "#                                 remainder=\"passthrough\") \n",
    "\n",
    "# column_transform.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline step (Pipeline is for chaining steps together). Then passing through cross_val_score to check accuracy of adding 3 features instead of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - create the pipeline by passing through the PREPROCESSED data in column_transform into the LOGREG model\n",
    "\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# pipe = make_pipeline(column_transform, logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Now pass the entire pipeline into the CROSS VALIDATE MODEL.  Hopefully your accuracy score increased.  If\n",
    "# it did, then adding the 2 other features IMPROVED your model! (eg. 0.7777 versus 0.67)\n",
    "\n",
    "# cross_val_score(pipe, X, y, cv=5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validating an ENTIRE PIPELINE OF STEPS (not just a model), but preprocessing of data and model building. \n",
    "# This includes:\n",
    "\n",
    "# a) cross val will split the data, 5 fold split\n",
    "# b) after splitting the data, it will run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, what do you do now?  Use new data and run it through the model you just built to predict! using the model that was created in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - if you need to quickly create new data by randomly selecting from your TRAINING data (don't do this)\n",
    "\n",
    "# X_new = X.sample(5, random_state = 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - training the model in the pipeline \n",
    "\n",
    "# pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - predict!\n",
    "\n",
    "# pipe.predict(X_new)\n",
    "\n",
    "# NOTE: passing X_new only works because the PIPELINE INCLUDES the preprocessing phase of breaking down Sex, Embarked \n",
    "#       into numerical values (dummy encoding/oneHotEncoding).  If you didn't have this phase, it wouldn't be\n",
    "#       able to predict because it would still be in CATEGORICAL data (male/female)\n",
    "# The one line of code is DOING EVERYTHING!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
