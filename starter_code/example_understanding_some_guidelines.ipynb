{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step - a Guideline to ML\n",
    "#### https://towardsdatascience.com/random-forest-in-python-24d0893d51c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 State the question and determine required data\n",
    "    # Example: \"Can we predict the max temperature tomorrow for our city?” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Acquire the data in an accessible format\n",
    "    # About 80% is cleaning and retrieving data but can be reduced if you find high quality data\n",
    "    \n",
    "# Pandas is used for data manipulation\n",
    "#import pandas as pd\n",
    "\n",
    "# Read in data and display first 5 rows\n",
    "#features = pd.read_csv('filename.csv')\n",
    "#features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Identify and correct missing data points/anomalies as required\n",
    "\n",
    "#print('The shape of our features is:', features.shape)\n",
    "#round(features.describe, 2)\n",
    "#create charts to see if there is a regression\n",
    "#boxplots for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4a Prepare the data for the machine learning model\n",
    "\n",
    "# One-hot encode the data using pandas get_dummies\n",
    "# features = pd.get_dummies(features)\n",
    "# Display the first 5 rows of the last 12 columns\n",
    "# features.iloc[:,5:].head(5)\n",
    "\n",
    "#4b Separate the data into the FEATURES (What you're using to predict) and TARGET/LABELS (What you're trying to predict)\n",
    "#4c Convert the Pandas dataframes to Numpy arrays because that is the way the algorithm works\n",
    "\n",
    "# Use numpy to convert to arrays\n",
    "# import numpy as np\n",
    "# Labels are the values we want to predict\n",
    "# labels = np.array(features['actual'])\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "# features= features.drop('actual', axis = 1)\n",
    "# Saving feature names for later use\n",
    "# feature_list = list(features.columns)\n",
    "# Convert to numpy array\n",
    "# features = np.array(features)\n",
    "\n",
    "# 4d Use Skicit-learn to split data into training and testing sets. Print to make sure it split correctly\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "# print('Training Features Shape:', train_features.shape)\n",
    "# print('Training Labels Shape:', train_labels.shape)\n",
    "# print('Testing Features Shape:', test_features.shape)\n",
    "# print('Testing Labels Shape:', test_labels.shape)\n",
    "\n",
    "# 4e Note: There may be extra work involved such as removing outliers, imputing missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Establish a baseline model that you aim to exceed.  Example: We now have our goal! \n",
    "# If we can’t beat an average error of 5 degrees, then we need to rethink our approach.\n",
    "\n",
    "# # The baseline predictions are the historical averages\n",
    "# baseline_preds = test_features[:, feature_list.index('average')]\n",
    "# # Baseline errors, and display average baseline error\n",
    "# baseline_errors = abs(baseline_preds - test_labels)\n",
    "# print('Average baseline error: ', round(np.mean(baseline_errors), 2))\n",
    "# Average baseline error:  5.06 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Train the model on the TRAINING data.  Here we are using RandomForestRegressor model\n",
    "\n",
    "# # Import the model we are using\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# # Instantiate model with 1000 decision trees\n",
    "# rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# # Train the model on TRAINING data\n",
    "# rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Make predictions on the TEST data.  In this example, MSE is 3.83\n",
    "\n",
    "# # Use the forest's predict method on the TEST data\n",
    "# predictions = rf.predict(test_features)\n",
    "# # Calculate the absolute errors\n",
    "# errors = abs(predictions - test_labels)\n",
    "# # Print out the mean absolute error (mae)\n",
    "# print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 Compare predictions to the known test set targets and calculate performance metrics. How good is our model?\n",
    "# Depending on the model used, you can view MSE, score_accuracy, feature_importances etc.  In this example,\n",
    "# accuracy is 93.99%.  That means this model is able to predict Seattle's max temp for tomorrow with 94% accuracy!\n",
    "\n",
    "# # Calculate mean absolute percentage error (MAPE)\n",
    "# mape = 100 * (errors / test_labels)\n",
    "# # Calculate and display accuracy\n",
    "# accuracy = 100 - np.mean(mape)\n",
    "# print('Accuracy:', round(accuracy, 2), '%.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 If performance is not satisfactory, adjust the model, acquire more data, or try a different modeling technique\n",
    "# Hyperparameter tuning if needed will be done here.\n",
    "# https://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 Interpret model and report results visually and numerically\n",
    "#  The question is: How does this model arrive at the values? There are two approaches \n",
    "#  to get under the hood of the random forest: \n",
    "#  A) Visualizing a Single Decision Tree\n",
    "#  B) Variable Importances\n",
    "\n",
    "# # VISUALIZING A SINGLE DECISION TREE\n",
    "# # Limit depth of tree to 3 levels\n",
    "# rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3)\n",
    "# rf_small.fit(train_features, train_labels)\n",
    "# # Extract the small tree\n",
    "# tree_small = rf_small.estimators_[5]\n",
    "# # Save the tree as a png image\n",
    "# export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "# (graph, ) = pydot.graph_from_dot_file('small_tree.dot')\n",
    "# graph.write_png('small_tree.png');\n",
    "\n",
    "# VARAIBLE IMPORTANCES\n",
    "# # Get numerical feature importances\n",
    "# importances = list(rf.feature_importances_)\n",
    "# # List of tuples with variable and importance\n",
    "# feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# # Sort the feature importances by most important first\n",
    "# feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# # Print out the feature and importances \n",
    "# [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11 Rerun model with just the TOP feature importances.  Compare accuracy!  If it's the same,\n",
    "# then you could techically run with the top 2 features and ignore the rest!\n",
    "\n",
    "# # New random forest with only the two most important variables\n",
    "# rf_most_important = RandomForestRegressor(n_estimators= 1000, random_state=42)\n",
    "# # Extract the two most important features\n",
    "# important_indices = [feature_list.index('temp_1'), feature_list.index('average')]\n",
    "# train_important = train_features[:, important_indices]\n",
    "# test_important = test_features[:, important_indices]\n",
    "# # Train the random forest\n",
    "# rf_most_important.fit(train_important, train_labels)\n",
    "# # Make predictions and determine the error\n",
    "# predictions = rf_most_important.predict(test_important)\n",
    "# errors = abs(predictions - test_labels)\n",
    "# # Display the performance metrics\n",
    "# print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "# mape = np.mean(100 * (errors / test_labels))\n",
    "# accuracy = 100 - mape\n",
    "# print('Accuracy:', round(accuracy, 2), '%.')\n",
    "# Mean Absolute Error: 3.9 degrees.\n",
    "# Accuracy: 93.8 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 Visualizing your findings!\n",
    "\n",
    "#12a Visualizing your feature importances\n",
    "# # Import matplotlib for plotting and use magic command for Jupyter Notebooks\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# # Set the style\n",
    "# plt.style.use('fivethirtyeight')\n",
    "# # list of x locations for plotting\n",
    "# x_values = list(range(len(importances)))\n",
    "# # Make a bar chart\n",
    "# plt.bar(x_values, importances, orientation = 'vertical')\n",
    "# # Tick labels for x axis\n",
    "# plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "# # Axis labels and title\n",
    "# plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');\n",
    "\n",
    "#12b Visualizing the entire weather dataset with predictions highlighted\n",
    "# This way you can see if there are any outliers\n",
    "\n",
    "# # Use datetime for creating date objects for plotting\n",
    "# import datetime\n",
    "# # Dates of training values\n",
    "# months = features[:, feature_list.index('month')]\n",
    "# days = features[:, feature_list.index('day')]\n",
    "# years = features[:, feature_list.index('year')]\n",
    "# # List and then convert to datetime object\n",
    "# dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "# dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]\n",
    "# # Dataframe with true values and dates\n",
    "# true_data = pd.DataFrame(data = {'date': dates, 'actual': labels})\n",
    "# # Dates of predictions\n",
    "# months = test_features[:, feature_list.index('month')]\n",
    "# days = test_features[:, feature_list.index('day')]\n",
    "# years = test_features[:, feature_list.index('year')]\n",
    "# # Column of dates\n",
    "# test_dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "# # Convert to datetime objects\n",
    "# test_dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in test_dates]\n",
    "# # Dataframe with predictions and dates\n",
    "# predictions_data = pd.DataFrame(data = {'date': test_dates, 'prediction': predictions})\n",
    "# # Plot the actual values\n",
    "# plt.plot(true_data['date'], true_data['actual'], 'b-', label = 'actual')\n",
    "# # Plot the predicted values\n",
    "# plt.plot(predictions_data['date'], predictions_data['prediction'], 'ro', label = 'prediction')\n",
    "# plt.xticks(rotation = '60'); \n",
    "# plt.legend()\n",
    "# # Graph labels\n",
    "# plt.xlabel('Date'); plt.ylabel('Maximum Temperature (F)'); plt.title('Actual and Predicted Values');\n",
    "\n",
    "#12c To further diagnose the model, we can plot residuals (the errors) to see if our model has a tendency \n",
    "# to over-predict or under-predict, and we can also see if the residuals are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to improve ML model?\n",
    "##### https://towardsdatascience.com/improving-random-forest-in-python-part-1-893916666cd\n",
    "##### https://github.com/WillKoehrsen/Data-Analysis/tree/master/random_forest_explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are three general approaches for improving an existing machine learning model:\n",
    "##### Use more (high-quality) data and feature engineering\n",
    "##### Tune the hyperparameters of the algorithm\n",
    "##### Try different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIPS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Use RandomForest feature importance to help narrow important features for other models. Often with \n",
    "#  feature reduction, there will be a minor decrease in performance that must be weighed against the decrease \n",
    "#  in run-time. Machine learning is a game of making trade-offs, and run-time versus performance is usually \n",
    "#  one of the critical decisions.\n",
    "\n",
    "#2 Compare the same model with 1 year of data VERSUS same model with 6 years of data/more features. Does it \n",
    "#  improve model?\n",
    "\n",
    "#3 Too much data may not be good.  Balance of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING IF ADDITIONAL DATA PAYS OFF\n",
    "# # Make predictions on test data\n",
    "# predictions = rf_exp.predict(test_features)\n",
    "# # Performance metrics\n",
    "# errors = abs(predictions - test_labels)\n",
    "# print('Metrics for Random Forest Trained on Expanded Data')\n",
    "# print('Average absolute error:', round(np.mean(errors), 2), 'degrees.')\n",
    "# # Calculate mean absolute percentage error (MAPE)\n",
    "# mape = np.mean(100 * (errors / test_labels))\n",
    "# # Compare to baseline\n",
    "# improvement_baseline = 100 * abs(mape - baseline_mape) / baseline_mape\n",
    "# print('Improvement over baseline:', round(improvement_baseline, 2), '%.')\n",
    "# # Calculate and display accuracy\n",
    "# accuracy = 100 - mape\n",
    "# print('Accuracy:', round(accuracy, 2), '%.')\n",
    "# Metrics for Random Forest Trained on Expanded Data\n",
    "# Average absolute error: 3.7039 degrees.\n",
    "# Improvement over baseline: 16.67 %.\n",
    "# Accuracy: 93.74 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### While model parameters are learned during training, hyperparameters must be set by the data scientist before training.\n",
    "##### Evaluating each model only on the training set can lead to one of the most fundamental problems in machine learning: overfitting\n",
    "##### When a model performs highly on the training set but poorly on the test set, this is known as overfitting.\n",
    "##### The standard procedure for hyperparameter optimization accounts for overfitting through cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation technique using K-Fold CV method using SKlearn's RandomizedSearchCV method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In K-Fold CV, we further split our training set into K number of subsets, called folds\n",
    "# Using Scikit-Learn’s RandomizedSearchCV method, we can define a grid of hyperparameter ranges, \n",
    "# and randomly sample from the grid, performing K-Fold CV with each combination of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - # Clean your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - look at the available hyperparameters, we can create a random forest and examine the default values.\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# rf = RandomForestRegressor(random_state = 42)\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Look at parameters used by our current forest\n",
    "# print('Parameters currently in use:\\n')\n",
    "# pprint(rf.get_params())\n",
    "\n",
    "# Output:\n",
    "# Parameters currently in use:\n",
    "# {'bootstrap': True,\n",
    "#  'criterion': 'mse',\n",
    "#  'max_depth': None,\n",
    "#  'max_features': 'auto',\n",
    "#  'max_leaf_nodes': None,\n",
    "#  'min_impurity_decrease': 0.0,\n",
    "#  'min_impurity_split': None,\n",
    "#  'min_samples_leaf': 1,\n",
    "#  'min_samples_split': 2,\n",
    "#  'min_weight_fraction_leaf': 0.0,\n",
    "#  'n_estimators': 10,\n",
    "#  'n_jobs': 1,\n",
    "#  'oob_score': False,\n",
    "#  'random_state': 42,\n",
    "#  'verbose': 0,\n",
    "#  'warm_start': False}\n",
    "\n",
    "\n",
    "# # will focus on the following:\n",
    "# n_estimators = number of trees in the foreset\n",
    "# max_features = max number of features considered for splitting a node\n",
    "# max_depth = max number of levels in each decision tree\n",
    "# min_samples_split = min number of data points placed in a node before the node is split\n",
    "# min_samples_leaf = min number of data points allowed in a leaf node\n",
    "# bootstrap = method for sampling data points (with or without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Use Random Hyperparameter Grid - need to create a PARAMETER GRID to sample from during fitting. The\n",
    "#  benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.\n",
    "\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}\n",
    "# pprint(random_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Random Search Training.  We instantiate the random search and fit it like any Scikit-Learn model.\n",
    "# The most important arguments in RandomizedSearchCV are n_iter, which controls the number of different \n",
    "# combinations to try, and cv which is the number of folds to use for cross validation (we use 100 and 3 respectively). \n",
    "# More iterations will cover a wider search space and more cv folds reduces the chances of overfitting, but raising \n",
    "# each will increase the run time. Machine learning is a field of trade-offs, and performance vs time is one of \n",
    "# the most fundamental.\n",
    "\n",
    "\n",
    "# # Use the random grid to search for best hyperparameters\n",
    "# # First create the base model to tune\n",
    "# rf = RandomForestRegressor()\n",
    "\n",
    "# # Random search of parameters, using 3 fold cross validation, \n",
    "# # search across 100 different combinations, and use all available cores\n",
    "# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# rf_random.fit(train_features, train_labels)\n",
    "\n",
    "# REVIEW BEST PARAMETERS!!\n",
    "# rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewing results from Step 4\n",
    "\n",
    "# def evaluate(model, test_features, test_labels):\n",
    "#     predictions = model.predict(test_features)\n",
    "#     errors = abs(predictions - test_labels)\n",
    "#     mape = 100 * np.mean(errors / test_labels)\n",
    "#     accuracy = 100 - mape\n",
    "#     print('Model Performance')\n",
    "#     print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "#     print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "#     return accuracy\n",
    "\n",
    "# base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "# base_model.fit(train_features, train_labels)\n",
    "# base_accuracy = evaluate(base_model, test_features, test_labels)\n",
    "\n",
    "# Model Performance\n",
    "# Average Error: 3.9199 degrees.\n",
    "# Accuracy = 93.36%.\n",
    "\n",
    "# best_random = rf_random.best_estimator_\n",
    "# random_accuracy = evaluate(best_random, test_features, test_labels)\n",
    "\n",
    "# Model Performance\n",
    "# Average Error: 3.7152 degrees.\n",
    "# Accuracy = 93.73%.\n",
    "\n",
    "# print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n",
    "# Improvement of 0.40% (Quite dismal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Grid Search with Cross Validation - using GRIDSEARCHCV\n",
    "# Random search allowed us to narrow down the range for each hyperparameter. Now that we know where \n",
    "# to concentrate our search, we can explicitly specify every combination of settings to try. We do this \n",
    "# with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all \n",
    "# combinations we define. To use Grid Search, we make another grid based on the best values provided by random search\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# # Create the parameter grid based on the results of random search \n",
    "# param_grid = {\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': [80, 90, 100, 110],\n",
    "#     'max_features': [2, 3],\n",
    "#     'min_samples_leaf': [3, 4, 5],\n",
    "#     'min_samples_split': [8, 10, 12],\n",
    "#     'n_estimators': [100, 200, 300, 1000]\n",
    "# }\n",
    "\n",
    "# # Create a based model\n",
    "# rf = RandomForestRegressor()\n",
    "# # Instantiate the grid search model\n",
    "# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "#                           cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 - Fit the model using TRAIN DATA\n",
    "\n",
    "# # Fit the grid search to the data\n",
    "# grid_search.fit(train_features, train_labels)\n",
    "# grid_search.best_params_\n",
    "# {'bootstrap': True,\n",
    "#  'max_depth': 80,\n",
    "#  'max_features': 3,\n",
    "#  'min_samples_leaf': 5,\n",
    "#  'min_samples_split': 12,\n",
    "#  'n_estimators': 100}\n",
    "# best_grid = grid_search.best_estimator_\n",
    "# grid_accuracy = evaluate(best_grid, test_features, test_labels)\n",
    "# Model Performance\n",
    "# Average Error: 3.6561 degrees.\n",
    "# Accuracy = 93.83%.\n",
    "# print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))\n",
    "# Improvement of 0.50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
